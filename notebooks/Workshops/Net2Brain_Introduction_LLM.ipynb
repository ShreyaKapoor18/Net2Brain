{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhXmDhmt6Gd8"
      },
      "source": [
        "# Installing Net2Brain and Relevant Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPQh1MlxCtCT"
      },
      "outputs": [],
      "source": [
        "# !pip install -U git+https://github.com/cvai-roig-lab/Net2Brain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtrlzxH1sbn-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Restart Runtime\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWzawRYH6oQ_"
      },
      "source": [
        "# Net2Brain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"data/Net2Brain_Logo.png\" width=\"25%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn_2AvR-5ah7"
      },
      "source": [
        "__Net2Brain__ allows you to use one of over 600 Deep Neural Networks (DNNs) for your experiments comparing human brain activity with the activations of artificial neural networks. The DNNs in __Net2Brain__ are obtained from what we call different _netsets_, which are libraries that provide different pretrained models. \n",
        "\n",
        "__Net2Brain__ provides access to the following _netsets_:\n",
        "- [Standard torchvision](https://pytorch.org/vision/stable/models.html) (`Pytorch`).\n",
        "This netset is a collection of the torchvision models including models for image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection, video classification, and optical flow.\n",
        "- [Timm](https://github.com/rwightman/pytorch-image-models#models) (`Timm`). \n",
        "A deep-learning library created by Ross Wightman that contains a collection of state-of-the-art computer vision models.\n",
        "- [PyTorch Hub](https://pytorch.org/docs/stable/hub.html) (`Torchhub`). \n",
        "These models are accessible through the torch.hub API and are trained for different visual tasks. They are not included in the torchvision module.\n",
        "- [PyTorch Video](https://pytorch.org/docs/stable/hub.html) (`Pyvideo`). \n",
        "Offers models for video analysis, including action recognition and motion classification.\n",
        "- [Unet](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/) (`Unet`). \n",
        "Unet also is available through the torch.hub.API and is trained for abnormality segmentation in brain MRI.\n",
        "- [Taskonomy](https://github.com/StanfordVL/taskonomy) (`Taskonomy`). A set of networks trained for different visual tasks, like Keypoint-Detection, Depth-Estimation, Reshading, etc. The initial idea for these networks was to find relationships between different visual tasks.\n",
        "- [Slowfast](https://github.com/facebookresearch/pytorchvideo) (`Pyvideo`). \n",
        "These models are state-of-the-art video classification models trained on the Kinetics 400 dataset, acessible through the torch.hub API.\n",
        "- [CLIP](https://github.com/openai/CLIP) (`Clip`). \n",
        "CLIP (Contrastive Language-Image Pre-Training) is a vision+language multimodal neural network trained on a variety of (image, text) pairs.\n",
        "- [CorNet](https://github.com/dicarlolab/CORnet) (`Cornet`). \n",
        "A set of neural networks whose structure is supposed to resemble the one of the ventral visual pathway and therefore implements more recurrent connections that are commonplace in the VVS.\n",
        "- [Huggingface](https://huggingface.co/) (`Huggingface`). \n",
        "Features a broad range of advanced language models that deal with text-input.\n",
        "- [Yolo](https://github.com/ultralytics/yolov5) (`Yolo`). \n",
        "Includes fast, accurate YOLOv5 models for real-time object detection in images and video streams.\n",
        "- **Toolbox** (`Toolbox`). \n",
        "A set of networks that are implemented within Net2Brain.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**Net2Brain** consists of 4 main parts:\n",
        "1. **Feature Extraction**\n",
        "  > Handles input in the form of images, videos or text and extracts relevant features for analysis and saves them into .npz files.\n",
        "2. **Representational Dissimilarity Matrix (RDM) Creation**\n",
        "> Utilizes numpy arrays (.npz files) from the feature extraction process to create RDMs that quantify dissimilarities between data representations with different distance metrics.\n",
        "3. **Evaluation**\n",
        "> Provides a comprehensive suite of evaluation methods including Linear Encoding, Representational Similarity Analysis (RSA), and more, to assess and compare model performance\n",
        "4. **Plotting**\n",
        "> Offers advanced visualization tools to graphically display the results of various analyses, enhancing interpretability and presentation of findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqQdckZHWVR4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbm36FteFXql"
      },
      "source": [
        "\n",
        "# Step 0: Exploring the Toolbox - Model Taxonomy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhM7X2FXF2AI"
      },
      "outputs": [],
      "source": [
        "from net2brain.taxonomy import show_all_architectures\n",
        "from net2brain.taxonomy import show_all_netsets\n",
        "from net2brain.taxonomy import show_taxonomy\n",
        "from net2brain.taxonomy import print_netset_models\n",
        "\n",
        "from net2brain.taxonomy import find_model_like_name\n",
        "from net2brain.taxonomy import find_model_by_dataset\n",
        "from net2brain.taxonomy import find_model_by_training_method\n",
        "from net2brain.taxonomy import find_model_by_visual_task\n",
        "from net2brain.taxonomy import find_model_by_custom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TKygwloFdk0"
      },
      "source": [
        "To view a list of all available models along with the information on which netset they belong to, you can use the `print_all_models()` function to print them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCgzKdvJyzMQ"
      },
      "outputs": [],
      "source": [
        "show_all_architectures()\n",
        "show_all_netsets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRFVwUj5tiq"
      },
      "source": [
        "You can also inspect the models available from a particular _netset_ using the function `print_netset_models()`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNrc9U0kToEo"
      },
      "source": [
        "We also offer a comprehensive model taxonomy to help you find the most suitable model for your study. Each model in our toolbox has distinct attributes that cater to various research requirements. To facilitate your selection process, we provide a taxonomic overview of the models available.\n",
        "\n",
        "To see the available attributes, use the show_taxonomy function. You can then search for a model based on one or more attributes using the following functions:\n",
        "\n",
        "- `find_model_like(model_name)`\n",
        "- `find_model_by_dataset(attributes)`\n",
        "- `find_model_by_training_method(attributes)`\n",
        "- `find_model_by_visual_task(attributes)`\n",
        "- `find_model_by_custom([attributes], model_name)`\n",
        "\n",
        "This taxonomy system is designed to help you easily identify and choose the most appropriate model for your research needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO5e2wMlTItN"
      },
      "outputs": [],
      "source": [
        "show_taxonomy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTLc3ybMShcf"
      },
      "source": [
        "Or you can find a model by its name using the function `find_model_like()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t0J7tIrSerQ"
      },
      "outputs": [],
      "source": [
        "find_model_like_name('ResNet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFJtD7dUX5o2"
      },
      "source": [
        "The `find_model_by_dataset(attributes)` function enables you to search for models associated with a specific dataset, such as 'ImageNet', 'ImageNet 22K', or 'COCO'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggT2UcQ0SNXs"
      },
      "outputs": [],
      "source": [
        "find_model_by_dataset(\"Taskonomy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDPsOQ2X7TK"
      },
      "source": [
        "The `find_model_by_training_method(attributes)` function helps you discover models based on their training methodology, such as 'Supervised', 'Jigsaw', or 'NPID'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReNMFkHwSQI3"
      },
      "outputs": [],
      "source": [
        "find_model_by_training_method(\"SimCLR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIfluynEX84g"
      },
      "source": [
        "The `find_model_by_visual_task(attributes)` function allows you to search for models specifically trained for a particular visual task, such as 'Object Detection', 'Panoptic Segmentation', or 'Semantic Segmentation'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su3z7_0CSUwf"
      },
      "outputs": [],
      "source": [
        "find_model_by_visual_task(\"Panoptic Segmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aYEbqWkX-Ui"
      },
      "source": [
        "The `find_model_by_custom([attributes], model_name)` function enables you to search for models based on a combination of the attributes mentioned above. You can provide a list of attributes to filter the models, and optionally specify a particular model name to further refine your search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzeDdnGgSZg5"
      },
      "outputs": [],
      "source": [
        "find_model_by_custom([\"COCO\", \"Object Detection\"], model_name=\"fpn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q4Eb8lHWT8D"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example Study: Predictive capabiltiies of Large Language Models for the Visual Cortex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"data/Net2Brain_LE_Tutorial.png\" width=\"100%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we explore the **intersection of artificial intelligence and neuroscience**, specifically focusing on the **predictive capabilities of Large Language Models (LLMs)** and **Vision Transformers** in relation to the **Visual Cortex** of the human brain.\n",
        "\n",
        "For our experiment we use the **Net2Brain toolbox**, which facilitates the comparison of activations within deep neural networks (DNNs) to human brain activity in response to identical stimuli.\n",
        "\n",
        "By comparing the visual and language components of models like **CLIP**, along with other LLMs, we hope to shed light on well these systems can mimic, or perhaps even elucidate, the operations of the human visual cortex. While it is hypothesized that the visual component of CLIP may demonstrate a stronger correlation with EVC activity due to its direct engagement with visual stimuli, the potential explanatory power of LLMs regarding visual processing, especially in other brain areas, remains an open question.\n",
        "\n",
        "Through this example study, we hope to not only provide insights into the parallels between artificial and biological neural networks but also offer a guide on using the **Net2Brain toolbox**. This tutorial is designed to equip you with the knowledge and tools necessary to make your own explorations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Preparing the Dataset\n",
        "\n",
        "The **Net2Brain** toolbox provides access to a wide array of datasets suitable for various studies. For a comprehensive list of available datasets and their descriptions, please refer to the **Net2Brain dataset notebook**.\n",
        "\n",
        "For our current experiment we will use the **Natural Scenes Dataset (NSD)**. The NSD dataset is known for its rich collection of natural scene images, making it an excellent choice for our comparative study between human brain responses and model activations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading the NSD Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The **Net2Brain API** simplifies the process of acquiring datasets. To download the NSD dataset, you can use the following commands within the Net2Brain toolbox:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from net2brain.utils.download_datasets import DatasetAlgonauts_NSD\n",
        "from pprint import pprint\n",
        "\n",
        "Algonauts = DatasetAlgonauts_NSD()\n",
        "available_paths = Algonauts.load_dataset()\n",
        "pprint(available_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial we only want to conduct our experiment with one subject. Please select the subject of your choice down below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subject = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we will be using LLMs, we will need captions describing the images, that the participants have seen. Luckily NSD is derived from the COCO-Dataset. Net2Brain allows to download the COCO-Cpations via the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_stimuli = available_paths[f\"subj0{subject}_images\"]\n",
        "brain_data = available_paths[f\"subj0{subject}_rois\"]\n",
        "text_stimuli = available_paths[f\"subj0{subject}\"] + \"/training_split/training_text/\"\n",
        "\n",
        "#Algonauts.Download_COCO_Captions(image_stimuli, text_stimuli)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> For a comprehensive tutorial on how to use the datasets API, including details on additional datasets available, please refer to our detailed guide in the following notebook: [Exploring Net2Brain](../0_Exploring_Net2Brain.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilizing precomputed data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the time-intensive nature of Linear Encoding, especially within a tutorial setting, we recommend using precomputed data to expedite the learning process. (The actual download is further below right after Linear Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Step 2: Feature Extraction\n",
        "\n",
        "In this step we are going to extract the features of different Large Language Models (LLMs) and  CLIP's Vision Transformer (ViT), which will be later used to train a Linear Regression Model.\n",
        "\n",
        "### Model Inputs\n",
        "\n",
        "- **Large Language Models**: These models will use the original COCO Captions from the NSD images.\n",
        "- **Vision Models**: These models will use the original NSD (COCO) images, the same ones shown to human participants in the brain studies.\n",
        "\n",
        "### Models for Comparison\n",
        "\n",
        "We've chosen a mix of models for this study:\n",
        "\n",
        "- **CLIP ViT-B/32**: A Vision Transformer known for its strong performance in understanding images.\n",
        "- **CLIP RN50**: A different CLIP model to see how it stacks up against the ViT version.\n",
        "- **bert-base-uncased**: A basic but powerful language model.\n",
        "- **bert-large-cased-whole-word-masking**: A bigger version of BERT that's better at understanding the context of words.\n",
        "- **gpt2**: A well-known model for generating text, included here for a broader view of language model capabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting model features using `FeatureExtractor`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clip ViT-B/32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from net2brain.feature_extraction import FeatureExtractor\n",
        "extractor = FeatureExtractor(\"ViT-B/32\", \"Clip\", device=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This initializes the feature extractor and loads the model and any specified layers for extraction into the instance. To view the layers that are set to be extracted, you can execute `extractor.layers_to_extract`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "suggested_layers = extractor.layers_to_extract\n",
        "pprint(suggested_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the layers we suggested for extraction might not be well-suited for your experiemnts. To view a complete list of all available layers, you can use `extractor.get_all_layers()` and overwrite the `layers_to_extract` attribute with your desired subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extractor.get_all_layers()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using this knowledge, we have the option to select the layers we wish to extract from the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visual_layers = [\"visual.transformer.resblocks.0\",\n",
        "            \"visual.transformer.resblocks.1\",\n",
        "            \"visual.transformer.resblocks.2\",\n",
        "            \"visual.transformer.resblocks.3\",\n",
        "            \"visual.transformer.resblocks.4\",\n",
        "            \"visual.transformer.resblocks.5\",\n",
        "            \"visual.transformer.resblocks.6\",\n",
        "            \"visual.transformer.resblocks.7\",\n",
        "            \"visual.transformer.resblocks.8\",\n",
        "            \"visual.transformer.resblocks.9\",\n",
        "            \"visual.transformer.resblocks.10\",\n",
        "            \"visual.transformer.resblocks.11\"]\n",
        "\n",
        "text_layers = [\"transformer.resblocks.0\",\n",
        "            \"transformer.resblocks.1\",\n",
        "            \"transformer.resblocks.2\",\n",
        "            \"transformer.resblocks.3\",\n",
        "            \"transformer.resblocks.4\",\n",
        "            \"transformer.resblocks.5\",\n",
        "            \"transformer.resblocks.6\",\n",
        "            \"transformer.resblocks.7\",\n",
        "            \"transformer.resblocks.8\",\n",
        "            \"transformer.resblocks.9\",\n",
        "            \"transformer.resblocks.10\",\n",
        "            \"transformer.resblocks.11\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only visual layers\n",
        "extractor = FeatureExtractor(\"ViT-B/32\", \"Clip\", device=\"cpu\")\n",
        "extractor.extract(data_path=image_stimuli, save_path=f\"Tutorial_subj{subject}/clip_vit_b32_vision_feats\", consolidate_per_layer=False, layers_to_extract=visual_layers)\n",
        "\n",
        "# Only text layers:\n",
        "extractor = FeatureExtractor(\"ViT-B/32\", \"Clip\", device=\"cpu\")\n",
        "extractor.extract(data_path=text_stimuli, save_path=f\"Tutorial_subj{subject}/clip_vit_b32_text_feats\", consolidate_per_layer=False, layers_to_extract=text_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Alternative for multimodal models:**\n",
        "\n",
        "Clip takes both text and visual input. You can also create a folder with both stimuli and send the entire folder to CLIP and it will take care of the extraction!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bert-base-uncased\n",
        "extractor = FeatureExtractor(\"bert-base-uncased\", \"Huggingface\", device=\"cpu\")\n",
        "extractor.extract(data_path=text_stimuli, save_path=f\"Tutorial_subj{subject}/bert_base_feats\", consolidate_per_layer=False)\n",
        "\n",
        "\n",
        "# Bert-large-uncased\n",
        "extractor = FeatureExtractor(\"bert-large-cased-whole-word-masking\", \"Huggingface\", device=\"cpu\")\n",
        "extractor.extract(data_path=text_stimuli, save_path=f\"Tutorial_subj{subject}/bert_large_whole_word_feats\", consolidate_per_layer=False)\n",
        "\n",
        "\n",
        "# GPT2\n",
        "extractor = FeatureExtractor(\"gpt2\", \"Huggingface\", device=\"cpu\")\n",
        "extractor.extract(data_path=text_stimuli, save_path=f\"Tutorial_subj{subject}/gpt2_feats\", consolidate_per_layer=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Step 3: Linear Encoding\n",
        "\n",
        "In this step, we use Linear Encoding to compare model activations with human fMRI responses to the same images. This method trains a classifier using the model activations to see how well it can predict brain activity it has not seen before. This helps us understand the models' effectiveness in mimicking human brain processes.\n",
        "\n",
        "> **Note**: Due to its computational intensity, Linear Encoding can be time-consuming. For a more efficient tutorial experience, consider using precomputed results from the provided CSV files. You can start the download a few cells further down.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from net2brain.evaluations.encoding import Linear_Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This step has been automated to cycle through all available models efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary to loop through all models for which we extracted features from\n",
        "loop_dict = {f\"Tutorial_subj{subject}//bert_base_feats\": \"BERT_Base\",\n",
        "             f\"Tutorial_subj{subject}/bert_large_whole_word_feats\": \"BERT_large_whole_word\",\n",
        "             f\"Tutorial_subj{subject}//gpt2_feats\": \"GPT2\",\n",
        "             f\"Tutorial_subj{subject}/clip_vit_b32_text_feats\": \"CLIP_Text\",\n",
        "             f\"Tutorial_subj{subject}/clip_vit_b32_vision_feats\": \"CLIP_Vision\"}\n",
        "\n",
        "\n",
        "\n",
        "# Loop through our models\n",
        "for feat_path, model_name in loop_dict.items():\n",
        "    print(f\"Linear Encoding for {model_name} ...\")\n",
        "\n",
        "    # Call the linear encoding function\n",
        "    results_df_clip = Linear_Encoding(\n",
        "        feat_path=feat_path,\n",
        "        roi_path=brain_data,\n",
        "        model_name=model_name,\n",
        "        trn_tst_split=0.8,\n",
        "        n_folds=3,\n",
        "        n_components=100,\n",
        "        batch_size=100,\n",
        "        random_state=42,\n",
        "        return_correlations=True,\n",
        "        save_path=f\"Tutorial_LE_Results/subj{subject}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative: Load Precomputed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from net2brain.utils.download_datasets import Tutorial_LE_Results\n",
        "Tutorial_LE_Results.load_dataset()\n",
        "\n",
        "\n",
        "def open_results(file_path, roi_category=None):\n",
        "    \"\"\"\n",
        "    Loads a DataFrame from a CSV file and optionally filters it by an ROI category.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): The path to the CSV file.\n",
        "    - roi_category (str, optional): The category key from roi_mappings to filter by.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The loaded (and optionally filtered) DataFrame.\n",
        "    \"\"\"\n",
        "    # Load the DataFrame from the CSV file\n",
        "    dataframe = pd.read_csv(file_path)\n",
        "    \n",
        "    # If an ROI category is specified, filter the DataFrame\n",
        "    if roi_category:\n",
        "        # Define ROI mappings\n",
        "        roi_mappings = {\n",
        "            'prf-visualrois': ['V1', 'V2', 'V3', 'hV4'], \n",
        "            'floc-bodies': ['EBA', 'FBA-1', 'FBA-2', 'mTL-bodies'],\n",
        "            'floc-faces': ['OFA', 'FFA-1', 'FFA-2', 'mTL-faces', 'aTL-faces'],\n",
        "            'floc-places': ['OPA', 'PPA', 'RSC'],\n",
        "            'floc-words': ['OWFA', 'VWFA-1', 'VWFA-2', 'mfs-words', 'mTL-words']\n",
        "        }\n",
        "        \n",
        "        # Check if the specified category exists in roi_mappings\n",
        "        if roi_category not in roi_mappings:\n",
        "            raise ValueError(f\"Category '{roi_category}' not found in roi_mappings.\")\n",
        "        \n",
        "        # Filter the DataFrame\n",
        "        dataframe = dataframe.loc[dataframe['ROI'].str.split('_').str[0].isin(roi_mappings[roi_category])]\n",
        "    \n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "roi_category = 'prf-visualrois'  # If you want to display the data only for certain brain regions\n",
        "#roi_category = 'floc-bodies'\n",
        "# roi_category = 'floc-faces'\n",
        "# roi_category = 'floc-places'\n",
        "#roi_category = 'floc-words'\n",
        "\n",
        "# Select subject\n",
        "subject = 1 # Downloaded data only available for subject 1\n",
        "\n",
        "results_df_clip_vision = open_results(f\"Tutorial_LE_Results/subj{subject}/CLIP_Vision.csv\", roi_category)\n",
        "results_df_clip_text = open_results(f\"Tutorial_LE_Results/subj{subject}/CLIP_Text.csv\", roi_category)\n",
        "results_df_bert = open_results(f\"Tutorial_LE_Results/subj{subject}/BERT_Base.csv\", roi_category)\n",
        "results_df_bert_masking = open_results(f\"Tutorial_LE_Results/subj{subject}/BERT_large_whole_word.csv\", roi_category)\n",
        "results_df_gpt = open_results(f\"Tutorial_LE_Results/subj{subject}/GPT2.csv\", roi_category)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from net2brain.evaluations.plotting import Plotting\n",
        "\n",
        "\n",
        "plotter = Plotting([results_df_clip_vision, \n",
        "                    results_df_clip_text, \n",
        "                    results_df_bert, \n",
        "                    results_df_bert_masking,\n",
        "                    results_df_gpt])\n",
        "\n",
        "results_dataframe = plotter.plot_all_layers(metric=\"R\", columns_per_row=2, simplified_legend=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mixed captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In our analysis, we observed that while large language models (LLMs) didn't quite match the predictive accuracy of the vision-based model, they were still capable of capturing some patterns in brain data. This raises an important question: Are these correlations meaningful or coincidental? To delve deeper into this, we have a couple of strategies we could pursue. One approach is to initialize the models with random weights by using `FeatureExtractor(model, netset, pretrained=False)`. Another strategy involves shuffling the captions to see if the models maintain their predictive performance even when there's a mismatch between the captions and the brain data. To skip the step of feature extraction, we can just mix the extracted features!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "def mix_activation_files(base_folder):\n",
        "    # Iterate over subdirectories in the base folder\n",
        "    for subdir in tqdm(os.listdir(base_folder)):\n",
        "        if \"_feats\" in subdir:\n",
        "            source_folder = os.path.join(base_folder, subdir)\n",
        "            target_folder = os.path.join(base_folder, subdir.replace(\"_feats\", \"_mixed_feats\"))\n",
        "            \n",
        "            if not os.path.exists(target_folder):\n",
        "                os.makedirs(target_folder)\n",
        "            \n",
        "            files = os.listdir(source_folder)\n",
        "            shuffled_files = random.sample(files, len(files))\n",
        "            \n",
        "            for original, shuffled in zip(files, shuffled_files):\n",
        "                shutil.copy(os.path.join(source_folder, original), os.path.join(target_folder, shuffled))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_folder = \"Tutorial_subj1\"\n",
        "mix_activation_files(base_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linear Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loop_dict = {f\"Tutorial_subj{subject}//bert_base_mixed_feats\": \"BERT_Base_mixed\",\n",
        "             f\"Tutorial_subj{subject}/bert_large_whole_word_mixed_feats\": \"BERT_large_whole_word_mixed\",\n",
        "             f\"Tutorial_subj{subject}//gpt2_mixed_feats\": \"GPT2_mixed\",\n",
        "             f\"Tutorial_subj{subject}/clip_vit_b32_text_mixed_feats\": \"CLIP_Text_mixed\",}\n",
        "\n",
        "\n",
        "for feat_path, model_name in loop_dict.items():\n",
        "    print(f\"Linear Encoding for {model_name} ...\")\n",
        "\n",
        "    # Call the linear encoding function\n",
        "    results_df_clip = Linear_Encoding(\n",
        "        feat_path=feat_path,\n",
        "        roi_path=brain_data,\n",
        "        model_name=model_name,\n",
        "        trn_tst_split=0.8,\n",
        "        n_folds=3,\n",
        "        n_components=100,\n",
        "        batch_size=100,\n",
        "        random_state=42,\n",
        "        return_correlations=True,\n",
        "        save_path=f\"Tutorial_LE_Results/subj{subject}\"\n",
        "    )\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Precomputed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "roi_category = 'prf-visualrois'\n",
        "# roi_category = 'floc-bodies'\n",
        "# roi_category = 'floc-faces'\n",
        "# roi_category = 'floc-places'\n",
        "# roi_category = 'floc-words'\n",
        "\n",
        "# Select subject\n",
        "subject = 1 # Downloaded data only available for subject 1\n",
        "\n",
        "results_df_clip_vision_cls = open_results(f\"Tutorial_LE_Results/subj{subject}/CLIP_Vision.csv\", roi_category) # Normal CLIP for comparison\n",
        "results_df_clip_text_mix = open_results(f\"Tutorial_LE_Results/subj{subject}/CLIP_Text_mixed.csv\", roi_category)\n",
        "results_df_bert_mix = open_results(f\"Tutorial_LE_Results/subj{subject}/BERT_Base_mixed.csv\", roi_category)\n",
        "results_df_bert_masking_mix = open_results(f\"Tutorial_LE_Results/subj{subject}/BERT_large_whole_word_mixed.csv\", roi_category)\n",
        "results_df_gpt_mix = open_results(f\"Tutorial_LE_Results/subj{subject}/GPT2_mixed.csv\", roi_category)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from net2brain.evaluations.plotting import Plotting\n",
        "\n",
        "\n",
        "plotter = Plotting([results_df_clip_vision_cls, \n",
        "                    results_df_clip_text_mix, \n",
        "                    results_df_bert_mix, \n",
        "                    results_df_bert_masking_mix,\n",
        "                    results_df_gpt_mix])\n",
        "\n",
        "results_dataframe = plotter.plot_all_layers(metric=\"R\", columns_per_row=2, simplified_legend=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PhXmDhmt6Gd8"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
